# -*- coding: utf-8 -*-
"""save_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11hh2Fib20Czf7QzI5PpXEmjPs2uvFlGG
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
import joblib

# 1. Load Data
# ASSUMPTION: Your dataset file is named 'alzheimers_disease_data.csv' and is in the same directory.
try:
    df = pd.read_csv('alzheimers_disease_data.csv')
except FileNotFoundError:
    print("Error: 'alzheimers_disease_data.csv' not found. Please ensure the file is in the same directory.")
    exit()

# 2. Define Features and Target
TARGET = 'Diagnosis'
DROPPED_COLUMNS = ['PatientID', 'DoctorInCharge']
FEATURES = [col for col in df.columns if col not in [TARGET] + DROPPED_COLUMNS]

X = df[FEATURES]
y = df[TARGET]

# 3. Handle Missing Values (Imputing mean/mode is a common practice if not explicitly shown)
# For simplicity, we'll use mean imputation for all columns if there are any missing values,
# as the notebook focuses on scaling/training.
X = X.fillna(X.mean())
# NOTE: In a production app, you should save a separate Imputer object as well.

# 4. Split Data (Needed to fit the scaler and model correctly)
X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. Fit and Save the StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

joblib.dump(scaler, 'scaler.pkl')
print("StandardScaler saved as scaler.pkl")

# 6. Train and Save the Random Forest Model
# This is the model with the best performance (0.9085 accuracy)
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_scaled, y_train)

joblib.dump(rf_model, 'random_forest_model.pkl')
print("Random Forest Model saved as random_forest_model.pkl")

print("\nModel saving complete. You can now run the Streamlit app.")
