# -*- coding: utf-8 -*-
"""ALZHEMERS_(2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bj3L5KyBJPPhOF0gOH8o0aTCGXeaMpn1

# ***Alzheimer's Disease Classification***

**OBJECTIVE:** The variable to be predicted is the Diagnosis status (0 = No Alzheimer's, 1 = Alzheimer's).

Therefore, this is a Classification project.

The primary goal is to analyze patient demographic, lifestyle, medical, clinical, and cognitive variables to understand factors associated with Alzheimer's Disease and build models that can predict the likelihood of diagnosis.


**PROBLEM STATEMENT:** Alzheimer’s Disease diagnosis depends on multiple interacting factors such as age, lifestyle, medical history, clinical measurements, and cognitive assessments. Identifying the most influential variables and understanding their patterns is essential for early detection and effective intervention.

# SECTION 1: Load Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("alzheimers_disease_data.csv")
df.head()

"""# Section 2 — Dataset Overview

This section checks:
- Shape of data
- Columns & data types
- Duplicate rows
- Basic statistical summary

"""

print("Shape:", df.shape)

print("\nColumn List:\n", df.columns.tolist())

print("\nDataset Info:")
df.info()
df.describe().T

"""## Interpretation: Data Overview
- The dataset contains **2,149 rows and 35 columns**, meaning we have a large number of features covering demographics, lifestyle, medical history, cognitive assessments, and symptoms.
- There are **no missing values**, which makes data cleaning easier.
- Most columns are **numeric**, except `DoctorInCharge`, which does not add analytical value.
- All features have valid ranges and datatypes, indicating the dataset is clean and ready for analysis.

# Section 3 - Check Duplicates

## Checking for Duplicate Records
Duplicates can distort analysis. We check and remove them if found.
"""

duplicates = df.duplicated().sum()
print("Number of duplicate rows:", duplicates)

# Remove duplicates if any
if duplicates > 0:
    df = df.drop_duplicates()

"""## Checking for Missing values"""

missing = df.isnull().sum().sort_values(ascending=False)

print("\nMissing Values Count:")
print(missing[missing > 0])

"""## Dropping Irrelevant Columns
- `DoctorInCharge` is constant ("XXXConfid") → no predictive value.

"""

df = df.drop(columns=['DoctorInCharge'])

"""## Numerical and Categorical Columns Identification"""

num_cols = df.select_dtypes(include=np.number).columns.tolist()
cat_cols = [c for c in df.columns if df[c].nunique() <= 20 and c not in num_cols]
print("\nNumeric Columns:", len(num_cols))
print("\nCategorical-like Columns:", cat_cols)

"""## Interpretation: Duplicate Records
- The output shows **0 duplicate rows**, meaning the dataset does not contain repeated entries.
- This confirms data quality is good and no removal is needed at this stage.

## Interpretation: Dropping Irrelevant Column
- The column `DoctorInCharge` has the same value ("XXXConfid") for all patients.
- Since it provides **no useful information for analysis or prediction**, it is safely removed.

# Section 4 — Univariate Analysis (Numeric)
"""

numeric_cols = df.select_dtypes(include=['int64','float64']).columns

df[numeric_cols].hist(figsize=(18, 18), bins=30)
plt.suptitle("Distribution of Numerical Features", fontsize=20)
plt.tight_layout()
plt.show()

"""## Interpretation: Numeric Feature Distributions
- Most numerical variables show **normal or slightly skewed distributions**.
- No extreme abnormalities or strange patterns are seen, meaning the data is realistically distributed.
- These plots help us understand variability and detect potential outliers.

# Section 5 — Categorical Feature Distribution
We check counts of each categorical variable.
"""

categorical_cols = ['Gender','Ethnicity','EducationLevel','Smoking','FamilyHistoryAlzheimers',
                    'CardiovascularDisease','Diabetes','Depression','HeadInjury',
                    'Hypertension','MemoryComplaints','BehavioralProblems','Confusion',
                    'Disorientation','PersonalityChanges','DifficultyCompletingTasks',
                    'Forgetfulness','Diagnosis']

for col in categorical_cols:
    plt.figure(figsize=(5,4))
    sns.countplot(x=df[col])
    plt.title(f"Distribution of {col}")
    plt.show()

"""## Interpretation: Categorical Feature Counts
- Gender, EducationLevel, and other binary variables show a balanced distribution.
- Diagnosis counts show how many patients have Alzheimer's vs. do not.
- These distributions help us understand whether the dataset is **balanced or imbalanced**, which is important for modeling.

# Section 6 — Correlation Heatmap
Shows how numerical variables relate to each other and with diagnosis.
"""

plt.figure(figsize=(20, 14))
corr = df.corr()
sns.heatmap(corr, cmap='coolwarm', annot=False)
plt.title("Correlation Heatmap")
plt.show()

"""## Interpretation: Correlation Heatmap
- Variables related to **clinical measurements** (blood pressure, cholesterol levels) show moderate correlations with each other.
- No harmful multicollinearity (correlation > 0.9), meaning models like Logistic Regression can still work without issues.

# Section 7 — Bivariate Analysis w.r.t Diagnosis
We examine how each variable differs for:
- Diagnosis = 0 (No Alzheimer's)
- Diagnosis = 1 (Alzheimer's)
"""

for col in numeric_cols:
    if col != "Diagnosis":
        plt.figure(figsize=(6,4))
        sns.boxplot(x=df['Diagnosis'], y=df[col])
        plt.title(f"{col} vs Diagnosis")
        plt.show()

"""## Interpretation: Numeric Features vs Diagnosis
- Patients diagnosed with Alzheimer's generally show:
  - **Lower MMSE scores**
  - **Lower ADL & FunctionalAssessment scores**
  - Slightly different clinical measurement patterns
- Clear separation in cognitive scores confirms that cognitive metrics are **important indicators** of Alzheimer's.

"""

for col in categorical_cols:
    if col != "Diagnosis":
        ct = pd.crosstab(df[col], df['Diagnosis'], normalize='index')
        ct.plot(kind='bar', figsize=(6,4))
        plt.title(f"{col} Relationship with Diagnosis")
        plt.ylabel("Proportion")
        plt.show()

"""## Interpretation: Categorical Features vs Diagnosis
- Conditions like **Hypertension, Diabetes, Depression, and MemoryComplaints** show different proportions across Diagnosis groups.
- Lifestyle factors such as **Smoking** and **AlcoholConsumption** show minor differences.
- These patterns help identify which binary conditions are associated with higher Alzheimer’s likelihood.

# Section 8 — Outlier Detection
We detect outliers using the IQR method.
"""

def detect_outliers(col):
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower) | (df[col] > upper)]
    return outliers.shape[0]

for col in numeric_cols:
    print(col, ":", detect_outliers(col))

"""## Interpretation: Outlier Detection
- Some clinical measurements (e.g., cholesterol and triglycerides) show a few outliers, which is **normal** for medical data.
- Outliers do not appear extreme enough to distort results, so we can either keep them or cap them later during modeling.

# Summary:

**Strong predictors of Alzheimer’s:**

- MMSE score

- Functional assessment

- ADL score

- Memory complaints

- Behavioral issues

- Forgetfulness and disorientation
"""



"""## Section 9 — Model Training: Logistic Regression

Now, let's prepare the data and train a Logistic Regression model to predict Alzheimer's disease diagnosis.
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import numpy as np

# Define features (X) and target (y)
X = df.drop('Diagnosis', axis=1)
y = df['Diagnosis']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Identify numerical columns for scaling (excluding 'PatientID' which is an identifier)
# Ensure 'PatientID' is not scaled as it's an identifier
scale_cols = [col for col in X_train.select_dtypes(include=np.number).columns if col not in ['PatientID']]

# Initialize StandardScaler
scaler = StandardScaler()

# Fit on training data and transform both training and testing data
X_train[scale_cols] = scaler.fit_transform(X_train[scale_cols])
X_test[scale_cols] = scaler.transform(X_test[scale_cols])

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""Now that the data is split, we will train a Logistic Regression model and evaluate its performance."""

# Initialize and train the Logistic Regression model
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
print("\nModel Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""## Section 10 — Model Training: Decision Tree Classifier

Let's now train and evaluate a Decision Tree Classifier to predict Alzheimer's disease diagnosis.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the Decision Tree Classifier model
dtc_model = DecisionTreeClassifier(random_state=42)
dtc_model.fit(X_train, y_train)

# Make predictions on the test set
dtc_y_pred = dtc_model.predict(X_test)

# Evaluate the model
print("\nDecision Tree Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, dtc_y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, dtc_y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, dtc_y_pred))

"""## Section 11 — Model Training: Random Forest Classifier

Next, we'll train and evaluate a Random Forest Classifier to predict Alzheimer's disease diagnosis.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the Random Forest Classifier model
rfc_model = RandomForestClassifier(random_state=42)
rfc_model.fit(X_train, y_train)

# Make predictions on the test set
rfc_y_pred = rfc_model.predict(X_test)

# Evaluate the model
print("\nRandom Forest Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, rfc_y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, rfc_y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, rfc_y_pred))

"""## Section 12 — Model Training: Support Vector Machine (SVM) Classifier

Next, we'll train and evaluate a Support Vector Machine (SVM) Classifier to predict Alzheimer's disease diagnosis.
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the SVM Classifier model
svm_model = SVC(random_state=42)
svm_model.fit(X_train, y_train)

# Make predictions on the test set
svm_y_pred = svm_model.predict(X_test)

# Evaluate the model
print("\nSVM Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, svm_y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, svm_y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, svm_y_pred))

"""## Section 13 — Model Training: Gaussian Naive Bayes Classifier

Next, we'll train and evaluate a Gaussian Naive Bayes Classifier to predict Alzheimer's disease diagnosis.
"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the Gaussian Naive Bayes model
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# Make predictions on the test set
nb_y_pred = nb_model.predict(X_test)

# Evaluate the model
print("\nGaussian Naive Bayes Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, nb_y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, nb_y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, nb_y_pred))

"""## Section 14 — Model Training: K-Nearest Neighbors (KNN) Classifier

Next, we'll train and evaluate a K-Nearest Neighbors (KNN) Classifier to predict Alzheimer's disease diagnosis.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the KNN Classifier model (using default n_neighbors=5)
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)

# Make predictions on the test set
knn_y_pred = knn_model.predict(X_test)

# Evaluate the model
print("\nKNN Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, knn_y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, knn_y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, knn_y_pred))

"""## Section 15 — Model Training: Gradient Boosting Classifier

Next, we'll train and evaluate a Gradient Boosting Classifier to predict Alzheimer's disease diagnosis.
"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the Gradient Boosting Classifier model
gbc_model = GradientBoostingClassifier(random_state=42)
gbc_model.fit(X_train, y_train)

# Make predictions on the test set
gbc_y_pred = gbc_model.predict(X_test)

# Evaluate the model
print("\nGradient Boosting Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, gbc_y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, gbc_y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, gbc_y_pred))

"""## Section 16 — Feature Importance for Gradient Boosting Classifier

Let's analyze the feature importance from our best-performing model, the Gradient Boosting Classifier, to understand which variables contribute most to the prediction of Alzheimer's disease.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Get feature importances from the trained Gradient Boosting model
feature_importances = gbc_model.feature_importances_

# Get feature names from the training data
feature_names = X_train.columns

# Create a DataFrame for better visualization
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
})

# Sort the features by importance in descending order
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df.head(20), palette='viridis')
plt.title('Top 20 Feature Importances from Gradient Boosting Classifier')
plt.xlabel('Importance')
plt.ylabel('Feature Name')
plt.tight_layout()
plt.show()

print("\nTop 10 most important features:\n", importance_df.head(10))



import joblib

# Export the trained Random Forest Classifier model
joblib.dump(rfc_model, "best_model.pkl")

# Export the fitted scaler
joblib.dump(scaler, "scaler.pkl")

print("Random Forest Classifier model saved as best_model.pkl")
print("Scaler saved as scaler.pkl")